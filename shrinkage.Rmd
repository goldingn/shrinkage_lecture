---
title: "Shrinkage methods"
subtitle: "for regression"
output:
  xaringan::moon_reader:
    css: [default, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache=TRUE)
```

# Motivation

### more accurate predictions

### more parsimonious model

---
# Motivation

#### more covariates than data $p > n$
 - Genome Wide Association Studies (GWAS)
  - lots of candidates, most of them unimportant
 - Feature engineering
  - lots of related covariates, want optimal mix
  
#### preventing overfitting
 - complex combinations of covariates
 - Feature engineering

#### identifying important predictors
 - 

---
# Why not stepwise selection?

#### not guaranteed optimal

#### sloooow

#### may not want to throw out a potentially useful predictor

---

# The idea

- *shrink* the coefficients toward 0 by adding a penalty on the size of $\beta$

- we want a model that fits the data, but we also don't want $\beta$ to be too big

---

# The idea

#### linear regression:

$$\hat{\beta} = \underset{\beta}{\mathrm{argmin}} \{ \mathrm{RSS}(\beta)\}$$

$$\mathrm{RSS}(\beta) = \sum_{i=1}^N(y_i - \hat{y}_i)^2$$
$$\mathbf{\hat{\mathbf{y}}} = \alpha + \mathbf{X}\beta$$

#### penalized regression:
$$\hat{\beta}^\mathrm{shrinkage} = \underset{\beta}{\mathrm{argmin}} \{ \mathrm{RSS}(\beta) + f(\beta)\}$$
<br>
- we don't care that this is a *biased estimator* of $\beta$

---

# Two penalties:
<br>
### Ridge regression
<br>
### The lasso

---
# Ridge regression


<br>
### penalise *sum of beta squared*
<br>

$$\mathrm{ridge \,\, penalty:} \,\,\,\,\,\, f(\beta) = \lambda \sum_{i=1}^p\beta^2$$
<br>
$$\hat{\beta}^\mathrm{ridge} = \underset{\beta}{\mathrm{argmin}} \{ \mathrm{RSS}(\beta) + \lambda \sum_{i=1}^p\beta^2\}$$
<!-- ### Solutions
linear regression: $\hat{\beta} = (\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{y}$
ridge regression: $\hat{\beta}^{\mathrm{ridge}} = (\mathbf{X}'\mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}'\mathbf{y}$-->

<!-- plot three panels with RSS, penalty, and sum -->

---
# Ridge regression

 interactive demo

---
# Ridge regression


#### as constrained optimisation

  
$$\hat{\beta}^\mathrm{ridge} = \underset{\beta}{\mathrm{argmin}} \{ \mathrm{RSS}(\beta)\} \,\,\,\, \mathrm{s.t.}\,\, \sum_{i=1}^p\beta^2 < c$$
<br>
<!--
if parameters within constrained region, no shrinkage.
otherwise they are brought to the constraint
-->

```{r ridge_constraint, echo = FALSE, fig.width = 15, fig.height = 5.5, dpi = 600, dev.args = list(pointsize = 25)}
source("constraint_app/functions.R")
data <- simulate_data()
fit <- fit_ls(data)
density <- density_surface(fit)

par(mfrow = c(1, 3))

# coefficients not shrunk
plot_setup()
add_density(density)

lambda <- 0.1
ridge <- plotting_data(lambda, type = "ridge")

add_contour(ridge$RSS)

c <- find_c(ridge$beta)
pts <- constraint_points(c, "ridge")
add_constraint(pts)
points(ridge$beta[1], ridge$beta[2], pch = 16)

title(bquote(lambda == .(lambda) ~~~~ c == .(round(c, 2))))

# coefficients not shrunk
plot_setup()
add_density(density)
ridge <- plotting_data(0, type = "ridge")
c <- find_c(ridge$beta)
pts <- constraint_points(c, "ridge")
add_constraint(pts)

title(bquote(lambda == 0 ~~~~ c == sqrt(sum(beta^2))))

# coefficients completely shrunk
plot_setup()
add_density(density)
c <- 0.05
pts <- constraint_points(c, "ridge")
add_constraint(pts)

title(bquote(lambda == infinity ~~~~ c == 0  ))
```

---
# The lasso
<br>
$$\mathrm{lasso \,\, penalty:} \,\,\,\,\,\, f(\beta) = \lambda \sum_{i=1}^p|\beta|$$
<br>
$$\hat{\beta}^\mathrm{ridge} = \underset{\beta}{\mathrm{argmin}} \{ \mathrm{RSS}(\beta) + \lambda \sum_{i=1}^p|\beta|\}$$

---
# ridge vs. lasso

<!-- how much difference could this possibly make? -->

```{r constraint_comparison, echo = FALSE, fig.width = 10, fig.height = 5.5, dpi = 600}
source("constraint_app/functions.R")
par(mfrow = c(1, 2))
plot_setup()
title(bquote("ridge constraint" ~ (L[2])))
pts <- constraint_points(2, "ridge")
add_constraint(pts)

plot_setup()
title(bquote("lasso constraint" ~ (L[1])))
pts <- constraint_points(2, "lasso")
add_constraint(pts)
```


---
# ridge vs. lasso

- the lasso shrinks coefficients to zero!

plot: shrinkage profiles for ridge vs. lasso

---
background-image: url("constraint_app.png")
background-position: center
background-repeat: no-repeat
background-size: 70%

### [the lasso shrinks coefficients to zero](https://goldingn.shinyapps.io/constraint_app/)

---
# practical issues

- selecting lambda
- standardizing

---
# Equivalences

### Bayesian ridge regression
$$\beta_i \sim N(0, ?)$$

<!--add figure of priors in profile-->

### Bayesian lasso
$$\beta_i \sim Laplace(0, ?)$$

equivalences in machine learning (weight decay & basis pursuit)

---
# other shrinkage methods

Least angle regression 
Elastic net


---
class: center, middle

# further reading



[`glmnet`]()

`r icon::fa("github", 5)`
### [`github.com/goldingn/shrinkage_lecture`](https://github.com/goldingn/shrinkage_lecture)

